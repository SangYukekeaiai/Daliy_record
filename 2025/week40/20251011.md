# Tasks
1. First check the latency of DRAM load. 1 Block.
2. Change the weight load method. (Checkpoint) (It will decrease the percentage of the weight loading) 4 Blocks.
3. Change input spine buffer to be double buffered. (Checkpoint) 6 Blocks.
   1. Change the simple_dram's read part.
   2. Think about the input spine buffer's structure, change it.
4. Change the weight to be double buffered. (Checkpoint) 6 Blocks.
5. Change the PE's output to be double buffered. (Checkpoint) 
6. Grab the data from repo 3. (6 Blocks)

## Timeline-schedule
1. 4:30-6:30 Task 2
2. 7:00-9:00 Task 3 part 1
3. 9:30-11:30 Task 3 part 2
4. 11:30-14:30 Eating
5. 14:30-16:30 Task 4 part 1
6. 17:00-I don't know when Task 4 finished.

### Task 2
1. Filter buffer:
    1. owned_tile_id: mark owned_tiles
2. LoadWeightFromDram(total_tiles, tile_id, layer_id):
    1. First check if the id is already in the owned_tile_id
       1. If it is, then we do not need to load it now;
       2. If not, 
          1. we clear the current  filter buffer and load the current tile_id;
          2. we update the owned_tile_id;
          3. Then we check if it is available to load the tile_id next to current tile_id;
             1. If it is, then we load the next tile_id(we compute the next_tile_id; ((current_tile_id +1) % total_tiles))
             2. If not, exit


### Task 3
#### Plan
1. Try to think about the construction of a conv layer and then double check with current work (1 Block)
2. Try to think about the core's work. (1 Block)
3. List what should be changed (1 Block)
4. Estimate the fix time of the whole systems.

1. Read the conv layer, do the distinction between the core behavior and layer behavior. (2 Blocks)
    1. Block 1: Low efficiency. It is so hard to start.
2. Read the core part, do the distinction between the core behavior and the other parts' behavior.
   
#### timeline:
1. First Block: clear all the member that belongs to Layer(Which is static. The dynamic part should be part of the core's staff)
2. Second Block: Implement the configure function and part of the generate batch function
3. Third Block, Forth Block, Fifth Block, Sixth Block: Finish the generation of the run_layer function.
For spine i from (0, 0) to (H, W)
    Load input spine for i ()
    For Tile j from 0 to total_tile - 1:
        Load weight of tile_idx
        Update the parameter got needed for tiled spine computation
        While (not finished)
            core.step_once()
        Update the control signal.
    core.StoreBackOutputToDRAM()



## Layer
### Member:
int layer_id_ = 0;

int C_in_ = 0, C_out_ = 0;
int H_in_ = 0, W_in_ = 0;
int H_out_ = 0, W_out_ = 0;
int Kh_ = 0, Kw_ = 0;
int Sh_ = 0, Sw_ = 0;
int Ph_ = 0, Pw_ = 0;

float threshold_ = 0.0f;
int   w_bits_      = 8;
bool  w_signed_    = true;
int   w_frac_bits_ = -1;
float w_scale_     = 1.0f;

batch_needed;
total_tiles;


a map of all the (h, w) <--> std::vector<std::vector<int>>

It should own a core.
It should own a dram.

### Function


### Input spine buffer:
#### Member
1. shadowed_cycle

#### Function
1. Switch from shadow to active.
2. Check the shadowed cycle.
3. Load the active data.
4. Update the latency of loading current data. Reset the shadowed cycle to 0.
5. Compute the next spines' loading info
    * h, w, 

