## Tasks
1. ~~Finish the fc_layer.~~
2. Finish the dram conversion.
3. Finish the data transfer.
4. Finish the real-data loading.

### timeline
1. ~~13:00-13:30 fc_layer~~
2. 13:30-16:30 dram conversion part 1
   1. ~~13:30-14:00 Block 1~~
   2. ~~14:00-14:30 Block 2~~
   3. 14:30-15:00 Block 3
   4. 15:00-15:30 Block 4
   5. 15:30-16:00 Block 5
   6. 16:00-16:30 Block 6
3. 17:00-20:00 dram conversion part 2
   1. 17:00-17:30 Block 1
   2. 17:30-18:00 Block 2
   3. 18:00-18:30 Block 3
   4. 18:30-19:00 Block 4
   5. 19:00-19:30 Block 5
   6. 19:30-20:00 Block 6

### Task 2
#### Goals
1. The conversion of the data to fit in the dram format.
2. Profile the data to determine the size of the hardware.

#### Next Steps
1. The storage format is like this:
Now I have the simple_dram format:
```cpp
// simple_dram.hpp
// All comments are in English.
#pragma once
#include <cstdint>
#include <vector>
#include <unordered_map>
#include <cstring>
#include <stdexcept>

namespace sf { namespace dram {

struct SpineMeta {
  uint32_t id = 0;        // logical spine id
  uint64_t addr = 0;      // byte address in DRAM space
  uint32_t size = 0;      // bytes
};

struct WeightTileMeta {
  uint32_t tile = 0;      // tile id
  uint64_t addr = 0;      // byte address in DRAM space
  uint32_t size = 0;      // bytes
};

struct LayerMeta {
  // Input spines (pre-filled by the builder)
  std::unordered_map<uint32_t, SpineMeta> input_spines;
  // Weight tiles (pre-filled by the builder)
  std::unordered_map<uint32_t, WeightTileMeta> weight_tiles;
  // Output write pointer (append-only region reserved for this layer)
  uint64_t output_write_ptr = 0;
  uint64_t output_region_begin = 0;
  uint64_t output_region_end   = 0;  // exclusive
  // Optional: recorded output segments by spine id (for later reads, if needed)
  std::unordered_map<uint32_t, std::vector<SpineMeta>> output_segments;
};

class SimpleDRAM {
public:
  explicit SimpleDRAM(uint64_t total_bytes)
    : mem_(total_bytes, 0) {}

  // Install or replace per-layer metadata (built elsewhere).
  void SetLayerMeta(uint32_t L, LayerMeta meta) {
    if (meta.output_write_ptr < meta.output_region_begin ||
        meta.output_write_ptr > meta.output_region_end)
      throw std::invalid_argument("output write ptr out of region");
    layers_[L] = std::move(meta);
  }

  // Load an input spine by logical id: memcpy into dst.
  // Returns number of bytes copied.
  uint32_t LoadInputSpine(uint32_t L, uint32_t spine_id, void* dst, uint32_t max_bytes) const {
    auto itL = layers_.find(L);
    if (itL == layers_.end()) throw std::out_of_range("layer not found");
    const auto& tbl = itL->second.input_spines;
    auto it = tbl.find(spine_id);
    if (it == tbl.end()) throw std::out_of_range("input spine not found");
    const SpineMeta& m = it->second;
    const uint32_t n = (m.size <= max_bytes) ? m.size : max_bytes;
    safe_copy_out(dst, m.addr, n);
    return n;
  }

  // Load a weight tile by tile id.
  uint32_t LoadWeightTile(uint32_t L, uint32_t tile_id, void* dst, uint32_t max_bytes) const {
    auto itL = layers_.find(L);
    if (itL == layers_.end()) throw std::out_of_range("layer not found");
    const auto& tbl = itL->second.weight_tiles;
    auto it = tbl.find(tile_id);
    if (it == tbl.end()) throw std::out_of_range("weight tile not found");
    const WeightTileMeta& m = it->second;
    const uint32_t n = (m.size <= max_bytes) ? m.size : max_bytes;
    safe_copy_out(dst, m.addr, n);
    return n;
  }

  // Store one output spine (append-only). Advances the layer's write pointer.
  // Also records the segment under its spine_id (optional for later inspection).
  void StoreOutputSpine(uint32_t L, uint32_t spine_id, const void* src, uint32_t bytes) {
    auto itL = layers_.find(L);
    if (itL == layers_.end()) throw std::out_of_range("layer not found");
    auto& meta = itL->second;

    if (meta.output_write_ptr + bytes > meta.output_region_end)
      throw std::overflow_error("output region full");

    safe_copy_in(meta.output_write_ptr, src, bytes);

    SpineMeta seg{spine_id, meta.output_write_ptr, bytes};
    meta.output_segments[spine_id].push_back(seg);
    meta.output_write_ptr += bytes; // update "next time store address := current + size"
  }

private:
  void safe_copy_out(void* dst, uint64_t addr, uint32_t n) const {
    if (addr + n > mem_.size()) throw std::out_of_range("read out of range");
    std::memcpy(dst, &mem_[static_cast<size_t>(addr)], n);
  }
  void safe_copy_in(uint64_t addr, const void* src, uint32_t n) {
    if (addr + n > mem_.size()) throw std::out_of_range("write out of range");
    std::memcpy(&mem_[static_cast<size_t>(addr)], src, n);
  }

  std::vector<uint8_t> mem_; // flat DRAM space
  std::unordered_map<uint32_t, LayerMeta> layers_;
};

}} // namespace sf::dram
```
I need to grab the conv layer and fc_layer's input (most likely it is the output of the previous lif layer, we need to make sure it is a spiking input), weight, and output, and convert it to the spine format in dram. The layout of input/weight/output in the dram is shown below.
It can be separate into two steps:
1. Grab the data from LoAS repo, generate a package that can be parsed in the dram data conversion function;
2. Implement the data conversion.
For the First part, It is better to integrate the script in the already existed package: spike_recorder
This is the directory tree of the package:
spike_recorder_pkg/
├── build/
├── spike_recorder/
│   ├── __pycache__/
│   ├── __init__.py
│   ├── feature_recorder.py
│   ├── model_inspector.py
│   ├── plotter.py
│   ├── recorder.py
├── spike_recorder.egg-info/
├── LICENSE
├── README.md
├── requirements.txt
└── setup.py
Implement everything in feature_recorder.py
The main function of LoAS is like this:
```python
import os
import time
import torch
import utils
import pickle
import torchvision
import torch.nn as nn
import numpy as np
import torchvision.transforms as transforms
from   torch.utils.data import Subset 
from spike_recorder import SpikeRecorder, SpikePlotter, SpikingInputRecorder
from spike_recorder import summarize_model, find_binary_conv_lif_with_loader
from tqdm import tqdm
from spikingjelly.clock_driven.functional import reset_net

#! The network architecture
from archs.cifarsvhn.vgg import vgg16_bn
from archs.cifarsvhn.resnet import ResNet19
from archs.cifarsvhn.alexnet import AlexNet

#! The helper functions
import utils_for_snn_lth
import config_profile
from utils import data_transforms, hook_fn
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

#! Global args
args = config_profile.get_args()

def main():
    print("------------ LoAS inference profiling for dual-sparse SNNs ----------")
    print('The profiling args are below:')
    print(args)

    train_transform, valid_transform = data_transforms(args)

    #! Feel free to extend this to new datasets
    if args.dataset == 'cifar10':
        trainset = torchvision.datasets.CIFAR10(root=args.data_dir, train=True, download=True, transform=train_transform)
        train_loader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=4)
        valset = torchvision.datasets.CIFAR10(root=args.data_dir, train=False, download=True, transform=valid_transform)
        single_test_set = Subset(valset, [0])  # or any index you want
        val_loader = torch.utils.data.DataLoader(single_test_set, batch_size=1, shuffle=False)
        # val_loader = torch.utils.data.DataLoader(valset, batch_size=args.batch_size, shuffle=False, pin_memory=True, num_workers=4)
        n_class = 10

    #! If you have your own model, you wanna bring them here
    if args.arch == 'vgg16':
        model = vgg16_bn(num_classes=n_class, total_timestep=args.timestep).cuda()
        model.load_state_dict(torch.load('./sample_ckpts/vgg16_final_dict.pth.tar', weights_only=False).state_dict())
    elif args.arch == 'resnet19':
        model = ResNet19(num_classes=n_class, total_timestep=args.timestep).cuda()
        model.load_state_dict(torch.load('./sample_ckpts/resnet19_final_dict.pth.tar', weights_only=False).state_dict())
    elif args.arch == 'alexnet':
        model = AlexNet(num_classes=n_class, total_timestep=args.timestep).cuda()
        model.load_state_dict(torch.load('./sample_ckpts/alexnet_final_dict.pth.tar'))

    _SP_PROFILE_ = args.profile
    
    
    #! Turn on the profiling
    if _SP_PROFILE_:
        n_layer = 0
        hook_fn.results = {}
        hook_fn.weighted_layers = {}
        hook_fn.weight_sparsity = {}
        for name, module in model.named_modules():
            if isinstance(module, nn.Conv2d):
                if n_layer == 0:
                    print('skip for thre first layer.')
                    n_layer+=1
                    continue
                print(f"Register hook function for Conv Layer{n_layer}.")
                hook = module.register_forward_hook(hook_fn(n_layer, args))
                hook_fn.results[n_layer] = [0]*(args.timestep+1)
                n_layer+=1
        
        n_layer = 0
        for name, module in model.named_modules():
            if isinstance(module, nn.Conv2d):
                tensor = module.weight.data.cpu().numpy()
                nz_count = np.count_nonzero(tensor)
                total_params = np.prod(tensor.shape)
                print(f"Weight Sparsity on Layer: {100-round((nz_count/total_params)*100,1)}")
                n_layer+=1
    
    comp1 = utils.print_nonzeros(model) #this shows weight sparsity!
    t1 = time.time()
    accuracy= test(model, val_loader)
    t2 = time.time()
    print(f"Time used for inference: {round(t2-t1,3)}s")
    print("Accuracy: ", accuracy)
    print("Weight Sparsity: ", 100-comp1)

    if _SP_PROFILE_:
        print("------Profiling the silent neuron sparsity------")
        network_sparsity_silent = 0.0
        total_size = 0.0
        for layer_name, result in hook_fn.weighted_layers.items():
            total_size += result
        for layer_name, result in hook_fn.results.items():
            num_samples = len(val_loader)
            profile_str = f"Average Percentage for spikes on Layer {layer_name}: "
            for i in range(args.timestep+1):
                avg_spk = result[i] / num_samples
                profile_str += (f"[{i} spikes {round(avg_spk*100,2)}] ")
                if i == 0:
                    network_sparsity_silent += (hook_fn.weighted_layers[layer_name]/total_size)*avg_spk 
            print(profile_str)
        print(f"Weight Spikes Sparsity Across Layers {round(network_sparsity_silent*100,2)}")
        

def test(model, test_loader):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.eval()
    test_loss = 0
    correct = 0

    with torch.no_grad():
        for data, target in tqdm(test_loader):
            batch = data.shape[0]
            data, target = data.to(device), target.to(device)
            if args.arch == 'resnet19':
                output = sum(model((data, args.n_mask)))
            else:
                output = sum(model(data, args.n_mask)) #! mask is sending in here to filter out the neurons with low firing activity.
            reset_net(model)
            _,idx = output.data.max(1, keepdim=True)  # get the index of the max log-probability
            
            correct += idx.eq(target.data.view_as(idx)).sum().item()
        accuracy = 100. * correct / len(test_loader.dataset)
    return accuracy


if __name__ == '__main__':
    main()
```

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
from collections import defaultdict
from typing import Dict, Any, Optional, List, Tuple

import torch
import torch.nn as nn
from spikingjelly.clock_driven import neuron


class SpikingInputRecorder:
    """
    Collect spiking (binary) inputs for ALL Conv2d and Linear layers across timesteps, along with:
      - the preceding LIF's threshold (v_threshold),
      - the target layer's weights (and bias if present).

    Design:
      - For each Conv2d / Linear module (the "target layer"), we heuristically find the most recent
        LIFNode that appears BEFORE it in named_modules() traversal.
      - We then register a forward hook on that LIF to record its outputs per timestep. These LIF
        outputs are the binary spikes that directly feed the target layer.
      - We also snapshot the target layer's weights (and bias) once at registration time, and
        read the LIF's v_threshold once.

    Time alignment:
      - Each target layer maintains its own timestep counter (tick) that increments every time its
        associated preceding LIF forwards. We bucket by t = tick % time_steps.

    Filtering:
      - Only "binary" (close to {0,1}) inputs are kept. Non-binary inputs are discarded.
      - Binary check uses a tolerance eps to account for float noise.

    Output:
      - After finalize(), each target layer has a dict: t -> Tensor (concatenated along batch).
      - print_dims() prints a clear summary of T, B, C, H, W (or B, N for Linear).
      - get() returns a dictionary of everything for downstream saving or formatting.

    Caveats:
      - The "preceding LIF" is found by linear traversal (heuristic). In branched graphs, this may not
        be the true graph predecessor, but usually works in common SNN stacks.
      - If a target layer has no preceding LIF (e.g., the very first Conv on raw images),
        it will be skipped (no spiking inputs to record).
    """

    def __init__(self, time_steps: int, save_cpu: bool = True, eps: float = 1e-6):
        """
        Args:
            time_steps: expected number of SNN timesteps in a rollout.
            save_cpu: move captured tensors to CPU immediately to reduce GPU memory footprint.
            eps: tolerance used to decide if a tensor is binary (close to {0,1}).
        """
        self.time_steps = int(time_steps)
        self.save_cpu = bool(save_cpu)
        self.eps = float(eps)

        # Per-layer storage keyed by target layer name
        self.layers: Dict[str, Dict[str, Any]] = {}
        # Hook handles to remove later
        self._handles: List[Any] = []

    # ------------------------ Module graph utilities ------------------------
    def _find_prev_lif_before(
        self, model: nn.Module, start_name: str
    ) -> Tuple[Optional[str], Optional[neuron.LIFNode]]:
        """
        Heuristically find the last LIFNode that appears BEFORE start_name in named_modules() order.
        Returns (lif_name, lif_module) or (None, None) if no such LIF exists.
        """
        last_lif_name, last_lif_module = None, None
        for n, m in model.named_modules():
            if n == start_name:
                break
            if isinstance(m, neuron.LIFNode):
                last_lif_name, last_lif_module = n, m
        return last_lif_name, last_lif_module

    def _is_binary_tensor(self, x: torch.Tensor) -> bool:
        """
        Return True if all elements in x are close to {0,1} within tolerance self.eps.
        Supports float/bool/int dtypes.
        """
        if x is None or not torch.is_tensor(x) or x.numel() == 0:
            return False
        if x.dtype == torch.bool:
            return True
        xt = x.detach().to(torch.float32)
        d = torch.minimum(torch.abs(xt - 0.0), torch.abs(xt - 1.0))
        return bool(torch.all(d <= self.eps).item())

    # ------------------------ Registration ------------------------
    def register(self, model: nn.Module):
        """
        Discover all Conv2d and Linear layers, locate their preceding LIFs, snapshot weights,
        and attach forward hooks on the LIFs to record binary spikes as inputs for the target layer.
        """
        # Build an index for quick lookup
        name_to_module = {n: m for n, m in model.named_modules()}

        # Traverse once to collect candidates
        for name, module in model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                target_type = "conv" if isinstance(module, nn.Conv2d) else "linear"

                # Find preceding LIF
                lif_name, lif_module = self._find_prev_lif_before(model, name)
                if lif_module is None:
                    # No preceding LIF -> no spiking inputs; skip this target layer
                    continue

                # Snapshot weights (and bias if present)
                with torch.no_grad():
                    w = module.weight.detach()
                    w = w.cpu().clone() if self.save_cpu else w.clone()
                    b = None
                    if getattr(module, "bias", None) is not None:
                        b = module.bias.detach()
                        b = b.cpu().clone() if self.save_cpu else b.clone()

                # Read LIF threshold once
                vth = getattr(lif_module, "v_threshold", None)
                try:
                    if vth is None:
                        thr = None
                    elif torch.is_tensor(vth):
                        thr = float(vth.detach().cpu().item())
                    else:
                        thr = float(vth)
                except Exception:
                    thr = None

                # Initialize per-layer storage
                self.layers[name] = {
                    "type": target_type,              # "conv" or "linear"
                    "module": module,                 # target module
                    "weight": w,                      # snapshot
                    "bias": b,                        # optional snapshot
                    "prev_lif_name": lif_name,        # associated preceding LIF name
                    "prev_lif_threshold": thr,        # float or None
                    "tick": 0,                        # timestep counter for this layer
                    "spikes_by_t": defaultdict(list), # t -> list[Tensor]
                    "binary_confirmed": None,         # None until first check; then True/False
                }

                # Attach a dedicated hook on the preceding LIF that writes into THIS layer's bucket
                def make_lif_fwd_hook(layer_key: str):
                    def _lif_fwd(module, inputs, output):
                        info = self.layers.get(layer_key, None)
                        if info is None:
                            return
                        y = output.detach()
                        if self.save_cpu:
                            y = y.cpu()

                        # Decide binary on first trigger; skip non-binary forever
                        if info["binary_confirmed"] is None:
                            info["binary_confirmed"] = self._is_binary_tensor(y)

                        if info["binary_confirmed"]:
                            t = info["tick"] % self.time_steps
                            info["spikes_by_t"][t].append(y)
                            info["tick"] += 1
                        # If not binary, do nothing (skip storing)
                    return _lif_fwd

                lif_hook = lif_module.register_forward_hook(make_lif_fwd_hook(name))
                self._handles.append(lif_hook)

    # ------------------------ Finalization ------------------------
    def finalize(self):
        """
        Concatenate each timestep bucket along batch dimension for every recorded layer.
        After this call, spikes_by_t[t] becomes a Tensor instead of a list of Tensors.
        """
        for layer, info in self.layers.items():
            buckets = info["spikes_by_t"]
            stacked: Dict[int, torch.Tensor] = {}
            for t, lst in buckets.items():
                if len(lst) == 0:
                    continue
                try:
                    stacked[t] = torch.cat(lst, dim=0)
                except Exception as e:
                    # Fall back to first element if concatenation fails
                    print(f"[Warn] {layer}: failed to cat timestep {t}: {e}")
                    stacked[t] = lst[0]
            info["spikes_by_t"] = stacked  # replace with dict[int -> Tensor]

    # ------------------------ Teardown ------------------------
    def remove_hooks(self):
        """Remove all hooks to avoid stale references and memory leaks."""
        for h in self._handles:
            h.remove()
        self._handles = []

    # ------------------------ Export / Introspection ------------------------
    def get(self, only_binary: bool = True) -> Dict[str, Any]:
        """
        Return a dictionary with all recorded data.

        Args:
            only_binary: if True, return only layers confirmed as binary inputs.
        """
        out: Dict[str, Any] = {
            "expected_timesteps": self.time_steps,
            "layers": {},
        }

        for name, info in self.layers.items():
            if only_binary and not info.get("binary_confirmed", False):
                continue
            out["layers"][name] = {
                "type": info["type"],
                "prev_lif_name": info["prev_lif_name"],
                "prev_lif_threshold": info["prev_lif_threshold"],
                "weight": info["weight"],
                "bias": info["bias"],
                "spikes_by_t": info["spikes_by_t"],  # dict[int -> Tensor]
            }
        return out

    def save(
        self,
        save_dir: str = "./spike_outputs/spiking_inputs",
        stem: str = "all_layers",
        only_binary: bool = True,
    ):
        """
        Save the package and per-layer blobs using torch.save.
        """
        os.makedirs(save_dir, exist_ok=True)
        pkg = self.get(only_binary=only_binary)
        torch.save(pkg, os.path.join(save_dir, f"{stem}_package.pt"))

        # Optionally save each layer individually for convenience
        for name, ld in pkg["layers"].items():
            safe = name.replace(".", "_")
            torch.save(ld["weight"], os.path.join(save_dir, f"{stem}_{safe}_weight.pt"))
            if ld["bias"] is not None:
                torch.save(ld["bias"], os.path.join(save_dir, f"{stem}_{safe}_bias.pt"))
            torch.save(ld["spikes_by_t"], os.path.join(save_dir, f"{stem}_{safe}_spikes_by_t.pt"))
            if ld["prev_lif_threshold"] is not None:
                torch.save(
                    ld["prev_lif_threshold"],
                    os.path.join(save_dir, f"{stem}_{safe}_threshold.pt"),
                )

    def print_dims(
        self,
        save_path: Optional[str] = None,
        only_binary: bool = True,
        max_timesteps: int = 6,
    ):
        """
        Print (and optionally save) a compact dimension report:
          - For each recorded layer (Conv2d / Linear):
            * #timesteps collected
            * For the first few timesteps: tensor shape decomposed as B, C, H, W (or B, N)

        Args:
            save_path: write the same text to a file if provided.
            only_binary: include only layers confirmed as binary inputs.
            max_timesteps: number of earliest timesteps to show per layer.
        """
        lines: List[str] = []
        lines.append(f"[SpikingInputRecorder] expected_timesteps={self.time_steps}")

        def _shape_str(t: torch.Tensor, ltype: str) -> str:
            if not torch.is_tensor(t):
                return "<?>"
            shp = tuple(t.shape)
            if ltype == "conv":
                # Expect [B, C, H, W]
                if len(shp) == 4:
                    B, C, H, W = shp
                    return f"[B={B}, C={C}, H={H}, W={W}]"
                # Fallback: print raw
                return str(shp)
            else:
                # linear: expect [B, N]
                if len(shp) == 2:
                    B, N = shp
                    return f"[B={B}, N={N}]"
                return str(shp)

        # Iterate layers
        for name, info in self.layers.items():
            if only_binary and not info.get("binary_confirmed", False):
                continue

            ltype = info["type"]
            tkeys = sorted(info["spikes_by_t"].keys())

            lines.append(f"- Layer: {name} ({ltype})")
            lines.append(f"  prev_lif: {info['prev_lif_name']} threshold={info['prev_lif_threshold']}")
            lines.append(
                f"  weight_shape: {tuple(info['weight'].shape) if info['weight'] is not None else None}"
            )
            if info["bias"] is not None:
                lines.append(f"  bias_shape : {tuple(info['bias'].shape)}")
            lines.append(f"  timesteps_collected: {len(tkeys)}")

            show = tkeys[: min(max_timesteps, len(tkeys))]
            for t in show:
                tensor_t = info["spikes_by_t"][t]
                lines.append(f"  t={t}: {_shape_str(tensor_t, ltype)}")
            if len(tkeys) > len(show):
                lines.append(f"  ... ({len(tkeys) - len(show)} more timesteps)")

        text = "\n".join(lines)
        print(text)

        if save_path is not None:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            with open(save_path, "w") as f:
                f.write(text + "\n")

```