## Tasks
1. Finish doing the smallest_ts_picker 4 Blocks
2. Finish the final output queues 8 Blocks
3. Finish the core 4 Blocks
4. Finish the layer 4 Blocks
5. Finish the real data generation

### timeline-schedule
1. 11:30-12:05 smallest_ts_picker part 1
2. 12:05-12:40 part 2
3. 12:40-13:25 part 3
4. 13:25-14:05 part 4
5. 14:05-14:40 output queue part 1
6. 14:40-15:25 output queue part 2
7. 15:25-16:05 part 3
8. 16:05-16:40 part 4
9. 16:40-17:15 part 5
10. 17:15-17:50 part 6
11. 17:50-18:25 part 7
12. 18:25-19:00 part 8

Try to think about which functions are needed. What parameters is needed. Where are they from.
1. For input spine buffer, it has two three functions:
    1. pre-load the logic spine id to the physical input spine buffer.
        * **Parameters needed:** An array of logical spine id of first batch. Layer id.
          *  **An array of logical spine id of first batch:** Layer's computation of a map: [[batch0: spine 0, spine 1, spine 2, ..., spine x], [batch1: spine a, spine b, ..., ], ..., [batch needed: spine x, spine y, spine z, ...]]--> Send to the core. Spine_buffer uses the member from the core.
          *  **Layer id:**  Layer's layer id.-->Send to the core.
        * **How to Implement:** Once recived the array of the logical spine id, 
          * Check the array size is less than or equals to the number of the physical buffer.
          * Iterate through physical buffer, use dram.LoadInputSpine(layer_id, spine_id, &input buffer[i], max_bytes = input_buffer.size) 
    2. In run function, if needed, load next batch's spine ids' to the physical spine buffer.
        * **Parameters needed:** An array of logical spine id of current batch. Layer id. current_batch_cursor.
          *  **current_batch_cursor.**: core will hold a current batch.
          *  **Layer id:**  Layer's layer id.-->Send to the core.
          *  **An array of logical spine id of current batch:** grab from core's map: map[batch_cursor]
          *  **How to implement:** 
            *  First check whether it is needed to load data from dram by checking whether the current batch cursor is less than batch needed.
            *  Check if all the buffer is empty, which is the pre-requisite of loading data.
            *  Check the array size is less than or equals to the number of the physical buffer.
            *  Iterate through physical buffer, use dram.LoadInputSpine(layer_id, spine_id, &input buffer[i], max_bytes = input_buffer.size)   
    3. Give the Entry with the smallest time steps to the min_finder_batch
        * **Parameters needed:** No
        * **How to implement:**
          * **Output: one Entry with smallest time steps**
          * Iterate through the input_spine_buffer, compare their header, pop the one with the smallest time steps.
2. For min_finder_batch, 
   1. it should have the following members:
      1. a pointer to the input_spine_buffer.
      2. a pointer to an array of Intermediate fifo.
      3. An entry to receive the picked entry.
      4. An true/false flag to determine if the last batch's first entry is pushed to the intermediate fifo.
   2. it should have two function: 
      1. run() which contains the two following functions:
         1. select the entry with the smallest time steps from the 16 header.
              * **Parameters needed:** a reference to input_spine_buffer's PopSmallestTsEntry(), an empty entry to store the poped Entry
                * a reference to input_spine_buffer's PopSmallestTsEntry(): from the reference of input_spine_buffer;
                *  **Entry:** internal variable
              * **How to Implement:** Use the function in the input spine buffer. 
         2. push the selected enty to the related intermediate fifo.
              * **Parameters needed:** current batch cursor, the reference of intermediate fifo's push(function), the Entry before.
                * current batch cursor: From core;
                * the reference of intermediate fifo's push: from intermediate_fifo.
                * The filled Entry.
              * **How to Implement:** 
                * First check the current cursor.
                * Then use the current batch cursor find the intermediate fifo in the array.
                * push the entry to that intermediate fifo 
         3. If the flag is already true, then we ignore it. else: If current_cursor == batch_needed, and the entry is successfully pushed to the intermediate fifo, then set the fifo to true.
      2. CanGlobalMegerWork()
            * **Parameters needed:** None
            * **How to Implement:** Return the flag.
3. For intermediate_fifo,
   1. it should have the following members:
      1. buf_[Capacities]. The Capacities should be part of common/constants.hpp
4. For Global Merger:
   1. Member:
      1. a reference to intermediate fifo
      2. a reference to min_finder_batch
   2. Function:
      1. It should have a function run()  
        * **Parameters needed:** 
          * A reference to MinFinderBatch's CanGlobalMergerWork() from min_finder_batch
          * An reference of Entry to receive the Entry.
        *  **How to implement:** 
          *  First check if it can work by using "CanGlobalMergerWork()"
          *  Then iterate through intermediate fifo (Actually it is the intermediate fifo belonging to the min_finder_batch), pop the smallest Entry.
5. For PE Array:
   1. Member:
      1. Entry to store the entry poped by GlobalMerger
      2. Weight Row Grabbed from the filter buffer
      3. An Array of PE
      4. An output array of Entries to store the output spiked entry for one time.
   2. Function:
      1. Iterate through the pe before the while loop of spinal flow, assign the output id and threshold to each pe.
        * **Parameters needed:** 
          * Threshold (From core)
          * tile index (from the core)
          * position number h, position number w (from core)
          * W (from the layer)
          * member: (pe array)
        *  **How to implement:** 
          *  Iterate through the the 128 pe, 
             *  Compute the output id := tile number * (h *W + w) * 128 + pe_idx, assign the pe.output_neuron_id with the computed value.
             *  set the threshold to pe.
      2. Get the entry poped by global merger: GetInputEntryFromGM():
        * **Parameters needed:** 
          * An Entry.
        *  **How to implement:** 
          *  Assign the member variable by the output of the global merger.
      3. Find the weight row for computation: GetWeightRow():
        * **Parameters needed:**
          * member: Entry->neuron_id
          * a reference to filter buffer: It will compute the row id based on the sent neuron_id (by function ComputeRowId), then use the function GetRow(), getting a list of weight.
          * member: row(A list of weight): receive the list of weight.
        * **How to implement:**
          * filter_buffer.GetRow(filter_buffer.ComputeRowId(Entry->neuron_id))  
      4. run:
        * **Parameters needed:** 
        *  **How to implement:** 
          *  GetInputEntryFromGM(), get the input entry
          *  GetWeightRow(), get the weight row
          *  Iterate through the pe, set the parameter into pe, do the computation. If it is spiked, push the generated entry to the output array of entries. 
6. PE:
   1. Member:
     * Vmem
     * Threshold
     * output_neuron_id
     * spiked (bool)
   2. Functions:
      1. RegisterOutputId()
        **Parameter needed:**
         * outputId (From pe_array)
        **How to Implement:**
         * Assign the outputId to its member
      2. SetThreshold()
        **Parameter needed:**
         * threshold (From pe_array)
        **How to Implement:**
         * Assign the threshold to its member
      3.  Process()
        **Parameter needed:**
         * time steps
         * weight
        **How to Implement:**
         * Vmem += weight 
         * If Vmem >= threshold, reset Vmem to 0, then set spiked to true, and generate an Entry <time steps, output neuron id>
7. For Filter Buffer:
   1. Member:
      1. An array of weight row of 128 weight, weight is uint8_t, the total size of the array is 4068 rows.
      2. outh
      3. outw
      4. outW
      5. Input channel number
   2. Function:
      1. UpdateParameter():
        * **Parameters needed:** 
          * h (From core)
          * w (From core)
          * W (From core)
          * Input channel number (From core)-->(From layer)
        * **How to implement:**
          * the member variables are directly assigned by the input parameter.
      2. ComputeRowId()
        * **Parameters needed:** 
          * A neuron id
        * **How to implement:**
          * Since the neuron id is like this: channel number * (h *W + w) +  pe_idx, we only need to do neuron_id % (h * W + w), and we get the channel index
          * with the channel index, we just compute channel index * (h * W + w), and get the row id.
      3. GetRow()
        * **Parameters needed:**
          * A row id
        * **How to implement:**
          * Return the row id's data by the row id
      4. LoadWeightFromDram(tile idx):
        * **Parameters needed:** 
          * tile idx (From core)-->(From Layer?) (not sure)
          * Layer idx (From Layer)
        * **How to implement:**
          * Use LoadWeightTile to load data.
8. Tiled output buffer
   1. Member(3 and 4's name needs to be changed, I am not a good namer):
      1. reference to pe.
      2. tiled index
      3. stalled latency of PE
      4. 8 big buffer of entries, each one represents one tile of spine
   2. Function:
      1. run():
        * **Parameters needed:** 
          * tile index (member? I am not sure if it is needed)
          * Input channel number (From core)-->(From layer)
        * **How to implement:**
          * First check if the stalled latency of pe is zero
            * if not, stalled latency of pe --, return true
            * if it is zero:
              * check if pe_array->output entry has outputs
                * if so, 
                  * set the pe->output_entries.size to the stalled latency of PE;
                  * copy paste the data from pe_array->output_entries to tile_buffer[tiled_indx]
                  * if succeed return true
                * if not, return false.
9. Output Sorter:
   1. it should have the following members:
      1. a pointer to the Tiled output buffer.
      2. a pointer to output spine
   2. it should have a function: 
      1. Sort():
        * **Parameters needed:** 
          * Member: a pointer to the Tiled output buffer.
          * Member: a pointer to output spine
        * **How to implement:**
          * Iterate through the header of the tiled output buffer, pick up the entry with the smallest time steps.
          * push it to the output spine.
10. output spine:
1. it should have the following members:
      1. a pointer to the DRAM.
      2. a spine id set by the core (from core)-->(from layer). (Computed by h * W + w)
   1. it should have a function: 
      1. Push Entries:
        * **Parameters needed:** 
          * An entry that is needed to push
        * **How to implement:**
          * Simple
      2. StoreOutputSpineToDRAM()
* **Parameters needed:** 
          * layer_id
          * spine_id(member)
          * its own address
          * its own size
        * **How to implement:**
          * using the api from simple_dram: StoreOutputSpine(uint32_t L, uint32_t spine_id, const void* src, uint32_t bytes)
10. Let's consider the core:
1. First, we need to check the valid signal:
   1. In the loop, they have these type of the stages:
      1. tiled output buffer: get the entry to the tiled output buffer
      2. pe array run
      3. min_finder_batch got the data from input spine buffer and drain it into the intermediate fifo
      4. input spine buffer load data from dram if needed.
Only 1, 2, and 3 need to have a vaild signal. The components will read the upstream's vaild signal to decide whether or not run in this cycle. The components will try to set the vaild signal on its own stage. The vaild signal should belong to the core or not?  


11. For layers, the construction of spine_batch should be:
   * Members needed:
     * h_out, w_out, padding, stride, Kh, Kw, H, W.
   * How to implement:
     * use the above parameter to compute the slide window of h_in, w_in. The spine_id = h_in * W + w_in.
     * batches_needed = Kh * Kw / number of physical spine buffer. divide the spine_id array by batches needed, put them in different batches.
