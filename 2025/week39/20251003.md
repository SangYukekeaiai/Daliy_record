## Tasks
1. Finish the simulation:
   1. ~~Finish the input spine buffer re-configure 2-3 block~~
   2. Finish the filter buffer's re-config. 2-3 Block.
   3. Change the output queue's part. 4-6 Block.
   4. Convert the raw data to dram. Run the simulation (Change the buffer size to infinitely large). 4 blocks. 
   5. Profile the 3 repos' data. 4 Blocks.
   6. Add the latency count part.4 Blocks.
   7. Do the latency statistics. 4 Blocks.
2. Do the Cache conversion.


### timeline schedule.
1. ~~16:15-16:45 Task 1.1 part 1~~
2. ~~16:45-17:15 Task 1.1 part 2~~
3. ~~17:15-17:45 part 3~~ Re-schedule
4. 17:45-18:15 Task 1.2 part 1
5. 18:15-18:45 Task 1.2 part 2
6. 18:45-19:15 Task 1.2 part 3
7. 19:45-20:00 Rest
8. 20:00-20:30 Task 1.3 part 1
9. 20:30-21:00 part 2
10. 21:00-21:30 part 3
11. 21:30-22:00 part 4


### Task 1.1
#### Goal 
Try to think about which functions are needed. What parameters is needed. Where are they from.
1. For input spine buffer, it has two three functions:
    1. pre-load the logic spine id to the physical input spine buffer.
        * **Parameters needed:** An array of logical spine id of first batch. Layer id.
          *  **An array of logical spine id of first batch:** Layer's computation of a map: [[batch0: spine 0, spine 1, spine 2, ..., spine x], [batch1: spine a, spine b, ..., ], ..., [batch needed: spine x, spine y, spine z, ...]]--> Send to the core. Spine_buffer uses the member from the core.
          *  **Layer id:**  Layer's layer id.-->Send to the core.
        * **How to Implement:** Once recived the array of the logical spine id, 
          * Check the array size is less than or equals to the number of the physical buffer.
          * Iterate through physical buffer, use dram.LoadInputSpine(layer_id, spine_id, &input buffer[i], max_bytes = input_buffer.size) 
    2. In run function, if needed, load next batch's spine ids' to the physical spine buffer.
        * **Parameters needed:** An array of logical spine id of current batch. Layer id. current_batch_cursor.
          *  **current_batch_cursor.**: core will hold a current batch.
          *  **Layer id:**  Layer's layer id.-->Send to the core.
          *  **An array of logical spine id of current batch:** grab from core's map: map[batch_cursor]
          *  **How to implement:** 
            *  First check whether it is needed to load data from dram by checking whether the current batch cursor is less than batch needed.
            *  Check if all the buffer is empty, which is the pre-requisite of loading data.
            *  Check the array size is less than or equals to the number of the physical buffer.
            *  Iterate through physical buffer, use dram.LoadInputSpine(layer_id, spine_id, &input buffer[i], max_bytes = input_buffer.size)   
    3. Give the Entry with the smallest time steps to the min_finder_batch
        * **Parameters needed:** No
        * **How to implement:**
          * **Output: one Entry with smallest time steps**
          * Iterate through the input_spine_buffer, compare their header, pop the one with the smallest time steps.

