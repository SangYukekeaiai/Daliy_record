# Tasks
1. ~~The workload table 2 Blocks~~
2. ~~The explanation of the Scoreboard working machanism 3 Blocks~~
3. The final Performance Table 2 Blocks.
4. The first Re-use distance part 2 Blocks.

#### Weight Cache Design Choice
在选择设计Cache的过程中，我们根据weight layout设计了第一版的Cache Configuration
1. Cache capacity Choice: Following the cadoSYS paper, the cache capacity should at least cover one full weight tile to avoid capacity evictions: C_in×KW×KH×128 elements (aligned with the DRAM layout). Since C_in​ varies by layer, I plan to sweep capacities such as 72 KB (C_in=64), 144 KB, 288 KB, and 576 KB.
2. Cache row size Choice: Match the DRAM line size for simplicity and alignment: 128 B per cache row.
3. Prefetch policy Choice: From our SNN spike analysis, if one channel is likely to spike, its neighboring channels have elevated spiking probability. When fetching weights for [c_in, h_in, w_in], prefetch weight of adjacent channels [c_in+1 ... c_in+4, h_in, w_in], i.e., 4 additional 128 B lines. W
4. Replacement policy Choice: We use a scoreboard eviction policy over time. At time step = 0, we use LRU policy, and once a cache line of cin is accessed, we update the score of this channel in scoreboard[0]. For t > 0, For each access, we update the score of scorboard[t] as before. If there is an eviction, we first check each cache lines' score the score in scoreboad[t - 1]. We pick the cache line with lowerest score to kick out. If more than two lines have the same score, we draw back to LRU policy.
5. Associativity Choice: Sweep from 4 ways to 32 ways.
6. workload的选择和使用方法：在此处我们使用来自LoAS Paper中的resnet19 以及 vgg16作为我们的测试基准， 其参数如下表所示。我们首先获取resnet19和vgg16中的多数Conv Layer和FC Layer（输入为Spiking input(binary)的Layer）的spiking input的数据，然后我们将其转换为spinalFlow所需的类型（input/output spine以及按照SpinalFLow weight layout排列的weight数据），将其存入simulator的DRAM部分中，然后分别通过运行使用含有/不含有我们设计的具有简单Neuromophic机制的Cache的spinalFlow。

The table better to be configed like this:
Layer Configuration, VGG 16
Cin = 64 Hin=Win=32 KH=KW=3 Cout=64 Hout=Wout=32 padding=1 stride=1, features.3 
Cin = 64 Hin=Win=16 KH=KW=3 Cout=128 Hout=Wout=16 padding=1 stride=1, features.7
Cin = 128 Hin=Win=16 KH=KW=3 Cout=128 Hout=Wout=16 padding=1 stride=1, features.10
Cin = 128 Hin=Win=8 KH=KW=3 Cout=256 Hout=Wout=8 padding=1 stride=1, features.14
Cin = 256 Hin=Win=8 KH=KW=3 Cout=256 Hout=Wout=8 padding=1 stride=1, features.17 feature.20
Cin = 256 Hin=Win=4 KH=KW=3 Cout=512 Hout=Wout=4 padding=1 stride=1, features.24
Cin = 512 Hin=Win=4 KH=KW=3 Cout=512 Hout=Wout=4 padding=1 stride=1, features.27 features.30
Cin = 512 Hin=Win=2 KH=KW=3 Cout=512 Hout=Wout=2 padding=1 stride=1, features.34 features.37 features.40

Layer Configuration, RESNET 19
Cin = 64 Hin=Win=32 KH=KW=3 Cout=64 Hout=Wout=32 padding=1 stride=2, layer1.0.conv1
Cin = 128 Hin=Win=16 KH=KW=3 Cout=128 Hout=Wout=16 padding=1 stride=1, layer1.0.conv2 layer1.1.conv1 layer1.1.conv2 layer1.2.conv1 layer1.2.conv2 
Cin = 128 Hin=Win=8 KH=KW=3 Cout=256 Hout=Wout=8 padding=1 stride=2, layer2.0.conv1
Cin = 256 Hin=Win=8 KH=KW=3 Cout=256 Hout=Wout=8 padding=1 stride=1, layer2.0.conv2 layer2.1.conv1 layer2.1.conv2 layer2.2.conv1 layer2.2.conv2
Cin = 256 Hin=Win=4 KH=KW=3 Cout=512 Hout=Wout=4 padding=1 stride=2, layer3.0.conv1 
Cin = 512 Hin=Win=4 KH=KW=3 Cout=512 Hout=Wout=4 padding=1 stride=1, features.27 layer3.0.conv2 layer3.1.conv1 layer3.1.conv2

### future work
1. 我们需要对不同Layer中Cache line的reuse-distance的分布进行更进一步的分析，reuse distance与layer参数之间的关系；
2. 我们需要对目前的weight访问顺序进行更细致地分析，首先从理论上分析在何处会存在较多的Cache miss现象，其是否是由于spinalflow的dataflow决定的；是否存在与neuromophic特性相关的优化方式； 并按照不同的granularity（time step wise， tile wise， output spine wise）统计Cache的hit rate和miss rate；
3. 我们需要trace Cache的访问，对eviction behavior进行分析，将eviction行为分类成bad eviction（被evicted的data在一定时间内又被使用而产生miss）和good eviction，并对其进行统计分布。观察当前的eviction policy在何时失败，失败的原因；
4. 根据上述分析，设计更完善的eviction policy，pre-fetch policy以及index mapping方式，使得Neuromophic Cache具有更高的efficiency。