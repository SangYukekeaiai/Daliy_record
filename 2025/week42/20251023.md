## Tasks
1. Finish the report
2. Finish the slides
1. intro
   1. overview
   2. problem statement
   3. challenges of existing solutions
   4. our proposal
   5. our contribution
2. background
   1. ~~Spike Neural Network~~
   2. ~~Spike patterns~~
   3. CNN workflow's order inference
   4. Architecture of SpinalFlow
      1. ~~Term of Spines 18:30-19:00.~~
      2. ~~Hardware organization 19:30-20:00.~~
      3. ~~Process of SpinalFlow 20:00-22:00 3 parts.~~
3. Technical details
   1. Layout and Access Pattern of the SpinalFlow
   2. Cache design details
      1. Cache eviction policy
      2. Cache line setting
      3. Cache pre-fetching policy
4. evaulation setup
5. evaluation
6. discussion of limitation of this work
7. conclusion

### timeline schedule
1. 13:10-13:40 Spike patterns graph insert, and the explanation
2. 13:50-14:20 CNN workflow's order explanation, and the graph of the table. Part 1
3. 14:30-15:00 CNN workflow Part 2
4. 15:10-15:40 CNN workflow part 3
5. 15:50-16:20 Architecture of Spinalflow: overview (based on the spinalflow paper)
6. 16:30-17:00 The concept of a spine
7. 17:10-17:40 The process step of spinalflow
8. 17:50-18:20 The process step of spinalflow part 2
9. 18:30-19:00 The process step of spinalflow part 3
10. 19:10-19:40 The workflow of spinalflow part 4
11. 19:50-20:20 Technical details: The layout and access pattern of spinalflow 
12. 20:30-21:00 Cache policy choice
13. 21:10-21:40 Cache line setting
14. 21:50-22:20 Cache pre-fetching policy


在开始介绍SpinalFlow架构前，我们需要通过Fig 7所示的Conv Layer来给出SpinalFlow中使用的明确定义。如图七所示，我们有Cin个大小为Hin x Win的input Feature map，和Cout 个Cin x KW x KH weight。当第一个Cin x KW x KH weight 作用在Cin个input feature map的第一个receptive field上（图七input feature map有颜色的区域）时，我们会在第一个output feature map上产生第一个output neuron。同理，当第二个Cin x KW x KH weight 作用在Cin个input feature map的第一个receptive field上（图七input feature map有颜色的区域）时，我们会在第二个output feature map上产生第一个output neuron。当我们遍历Cout个weight，使其作用在所有的input feature maps上时，我们就产生了在所有of map上的第一个neuron。我们把在所有feature map上同一个位置的neuron的集合称为Spine——如图中的input feature maps和output feature maps中，具有相同颜色的neuron的集合。SpinalFlow的计算即是Spine-based，或者换句话说，是Spine-stationary的，在每个layer的计算中，遍历计算得到所有的output spine。
**Hardware Organzation**
如同SpinalFlow所描述的架构那样，我们设计的cycle level SpinalFlow simulator大致分为四个部分：Input Spine Buffers， Filter Bffer，PE Arrays， Output Spine Buffer。我们将以生成一个output spine的过程为单位，对SpinalFlow的硬件组成进行解释，并通过一个简单的output spine生成的例子，讲解SpinalFlow的运行过程。

在PE array中，我们总共有128个PE，每个PE负责生成一个output featuremap中的一个neuron。比如，第一个PE对应第一个feature map中的neuron，第二个PE对应第二个feature map中的neuron...以此类推。如同Spike-CNN所示，每一个PE都包含一个accumulator和一个comparator。在某个time step如果input receptive field有input，他们会fed by与input相对应的weight，更新当前的membrane potential，然后进行LIF操作，将更新后的membrane potential与Vth进行比较，判断是否在该时刻产生output spike。

在input spine buffer中，我们将存储按照时间顺序排列的，与此次生成的output spine的receptive field内的所有的input spike event。如图8所示，每个input spike event以如下Entry形式表示。 每个Entry大小为5B，entry中的第一个元素是spike event的时间， 大小为1B，第二个元素是spike event对应的neuron id， 大小为4B，即input spike的位置信息（cin， hin， win）。

Global Buffer中存有计算该layer所需的所有filter buffer，每一行有128个weight，与128个output feature map中的output neuron一一对应。在获得input spine的neuron id后，我们通过解码获得相应的weight信息，并将在一行内的每一个weight输入PE Array中对应的PE。

以下是一个简化的SpinalFlow运行计算获得第一个output Spine的例子。

**Cyle 1 of generating output spine 1**： 在第一个cycle，我们从已经按照时间顺序排列的input buffer中pop第一个Entry。第一个Entry是（1， 18），代表了input neuron 18会在time step 1 spike。我们根据spike neuron id 19， 获得了与input neuron对应的第18行的128个kernel weights（为了方便理解，此处我们简化了从neuron id到weight row id的映射过程为一一映射） 。这128个weight被对应的PE所读取，从而更新每个PE中的membrane potential。然后每个PE将各自的membrane potential与threshold相比较，如果membrane potential大于threshold，我们会输出一个output spike event到该output spine， 其形式与input spike event一样为一个拥有两个元素的Entry， 第一个元素是spike time step, 第二个元素是neuron id。在本Cycle中，如图八所示，PE 1和PE 79在timestep 1同时产生了两个output spike event，在经过一个serializer（图中省略了serializer的表示）之后，这两个元素被存入output spine。



**Cyle 2 of generating output spine 1**： 第二个Cycle 与第一个Cycle类似。我们获得第二个Entry，第一个Entry是（1， 206），代表了input neuron 206会在time step 1 spike。我们根据neuron id获得filter buffer的第206行，然后将其128个weight分别送入对应的PE。PE在更新membrane potential之后与threshold比较，判断是否产生output spike event。在Cycle 2中，PE 128产生了output spike，记为(1, 128)，压入output spine buffer中。

**Last Cycle of generating output spine 1**： 在按时间顺序pop出最后的一个entry（12， 241）后，我们如之前的Cycle的流程一样，生成了output spine 1的最后一个entry （12， 2）至此，第一个output spine被完全生成完成了，需要被存储入DRAM中。


**Preparation of generating output spine 2**: 在我们将生成的output spine 1存入DRAM后，我们需要reset所有pe中的membrane potential。在我们开始output spine2 第一个cycle前，需要先准备生成output spine 2对应的sorted receptive field input。 如图十一所示，我们首先从DRAM中获得该receptive field中的16 个input spines （在图中是一个4 x 4的slide window）。其中每一个input spine已经被按照时间顺序排列好了。之后我们通过一个简单的比较树，merge-sort receptive field中所有的entries，最终得到按时间排列的output spine 2 对应的receptive field。在此处我们有16个input spine buffer。因为kernel大小的不同，input spine buffers有时会有闲置（当kernel大小为3 x 3时），有时会需要跑多个周期才能生成按时间排序的receptive field（当kernel为5 x 5或 7 x 7时）。

**Hardware Details**
图十二展示了我们设计的完整的SpinalFlow架构。其各部分的具体参数可见表1，spinalflow 的dataflow的pseudocode和生成一个output spine的流程分别如图13， 14所示.

早先的input buffer现在由input spine buffer，min finder batch， intermediate fifo和global merger四个部分组成。其中16个20KB的double-buffered input spine buffer用来顺序串行加载DRAM中的input spines；连接这16个buffer的min finder batch是第一级的comparator tree，用来生成按时间排序的input spine batches. 在kernel size较小的情况下，如kernel size = 3 x 3时，我们只需load 9个input spines，这9个input spines经过min finder batch之后，已经完全被sort好；可是当kernel size 大于16时，如 kernel size = 7 x 7时，我们无法一次性获得sorted receptive field。我们需要对每16条加载的input spine buffers进行第一次排序，并将中间结果存入intermediate fifo中。此处设计的intermediate fifo总共有四条，每条大小为320KB。等所有的所有的intermediate fifo都加载完成时，通过global merger，即第二层的comparator tree，我们才能对receptive field进行最终的排序，从而获得完全按时间顺序排序的receptive field。
Filter Buffer需要能装载整个layer的所有weight，是大小为2304KB，Layout为[tile id][cin][kh][kw]排列，每一行为128B的double buffer。在每一次global merger输出一个entry后，其根据entry的neuron id，计算出filter buffer row id，并将包含该row的每个byte分别送入对应的PE中。
PE array由128个PE组成，在计算中每个PE对应着一个output feature map内neuron的计算。由于每个time step有可能存在多个PE同时产生output spike event，因此我们在每个PE后面增加了一个能存2个entry（10B）的double buffer，之后通过serializer对其进行串行化。在由于有一些相对较大的Conv Layer的output channel 是128的倍数，且每一个生成的output spine是严格按照时间序列排序的，因此我们同样也需要对output spine的生成进行缓存。在将被排序好的output spine最终输入DRAM之前，我们设计了4个大小为20KB的tiled output buffer，用于至多处理output channel = 512的Conv Layer。在tiled output buffer之后，我们使用了sorter——第三级comparator tree，来最终将output spine按时间顺序从小到大排列，并存入output spine buffer。output spine buffer是大小为1KB的double buffer，负责将output spine存入DRAM。



对于FC layer，其处理思路与CONV layer类似，只是将视为其input/output的H = 1， W= 1，且filter的KH， KW=1的CONV layer即可。

对于本项目目标而言，我们需要根据SNN的特性，来设计出一个相比Accelerator自带的SRAM更efficient的neuromorphic cache。那么首先，我们需要对已有的SRAM中不同类型数据的Layout和Access Sequence进行分析，从而确定我们Cache的设计范围。

#### Subsection{spinalFlow's Layout and Access Sequence}
spinalFlow的input，weight和output的layout以及access pattern如下表所示。从表中我们可以清晰地得知，
1. input/output由spine组成，而每一条spine都是利用Spike input/output的稀疏性，通过entry的形式，对input/output进行压缩。1）其每一条spine长度大小不一，且是顺序访问（即访问完该spine的所有entry后才能访问下一条spine）；2）我们很难将spike的特性（如input 在t时刻spikes，则有较大概率在t+1时刻spikes）应用在input/output上。 cache很难被因此很难设计与input output的Cache。
2. weight的layout与传统的DNN中的layout类似，而且其很显然可以利用我们在之前提到的特性来设计Cache相应的eviction policy以及pre-fetch policy，获得比SRAM/SCRATCHPAD高得多的efficiency。
因此我们选择实现spinalflow的weight Cache。


#### Weight Cache Design Choice
在选择设计Cache的过程中，我们根据weight layout设计了第一版的Cache Configuration
1. Cache capacity Choice: Following the cadoSYS paper, the cache capacity should at least cover one full weight tile to avoid capacity evictions: C_in×KW×KH×128 elements (aligned with the DRAM layout). Since C_in​ varies by layer, I plan to sweep capacities such as 72 KB (C_in=64), 144 KB, 288 KB, and 576 KB.
2. Cache row size Choice: Match the DRAM line size for simplicity and alignment: 128 B per cache row.
3. Prefetch policy Choice: From our SNN spike analysis, if one channel is likely to spike, its neighboring channels have elevated spiking probability. When fetching weights for [c_in, h_in, w_in], prefetch weight of adjacent channels [c_in+1 ... c_in+4, h_in, w_in], i.e., 4 additional 128 B lines. W
4. Replacement policy Choice: We use a scoreboard eviction policy over time. At time step = 0, we use LRU policy, and once a cache line of cin is accessed, we update the score of this channel in scoreboard[0]. For t > 0, For each access, we update the score of scorboard[t] as before. If there is an eviction, we first check each cache lines' score the score in scoreboad[t - 1]. We pick the cache line with lowerest score to kick out. If more than two lines have the same score, we draw back to LRU policy.
5. Associativity Choice: Sweep from 4 ways to 32 ways.
6. workload的选择和使用方法：


### Remain tasks
1. ~~Finish the Workload table list~~
2. ~~Finish the Explanation on the scoreboad policy.~~
3. The end to end efficiency measurment


我们按照上述Cache的Configuration，使用workload在spinalFlow上进行了实验。其实验结果如图所示.
